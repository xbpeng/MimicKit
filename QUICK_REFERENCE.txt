================================================================================
MIMICKIT CODEBASE: QUICK REFERENCE GUIDE
================================================================================

PROJECT LOCATION:
  /home/ultron/team_files/emma/MimicKit

GENERATED DOCUMENTATION:
  1. CODEBASE_OVERVIEW.md (19 KB) - Complete technical overview
  2. KEY_INTEGRATION_POINTS.md (10 KB) - Implementation guidance
  3. This file - Quick reference

================================================================================
1. OVERALL STRUCTURE
================================================================================

MimicKit is a RL framework for physics-based motion imitation with 4 main parts:

  mimickit/
  ├── engines/          - Physics simulation (Isaac Gym wrapper)
  ├── envs/             - Training environments (hierarchical structure)
  ├── learning/         - RL agents and neural networks
  ├── anim/             - Motion representation and kinematics
  └── util/             - Helper functions
  
  data/
  ├── assets/           - Character definitions (XML/URDF)
  │   └── g1/           - G1 humanoid robot
  ├── motions/          - Reference motion clips (pickle files)
  │   └── g1/           - G1 motions: walk, run, spinkick, cartwheel, etc.
  ├── agents/           - Agent configuration files (YAML)
  └── envs/             - Environment configuration files (YAML)

================================================================================
2. KEY ALGORITHMS & IMPLEMENTATIONS
================================================================================

DeepMimic:   Phase-guided reference tracking with rewards
AMP:         Adversarial Motion Priors - adds discriminator network
ADD:         Adversarial Differential Discriminators - uses obs differences
PPO:         Proximal Policy Optimization - base RL algorithm
AWR:         Advantage-Weighted Regression - alternative RL

CLASS HIERARCHY:
  BaseEnv → SimEnv → CharEnv → DeepMimicEnv → AMPEnv → ADDEnv

AGENT HIERARCHY:
  BaseAgent → PPOAgent → AMPAgent → ADDAgent

================================================================================
3. WHERE TO ADD RANDOM FORCES
================================================================================

RECOMMENDED LOCATION: /mimickit/envs/deepmimic_env.py

KEY METHODS:
  - __init__()          - Parse config, setup body IDs
  - _apply_random_forces() - Sample and apply forces
  - step()              - Call force application before physics step

TARGET BODIES (G1):
  - torso_link (mass: 9.598 kg) - Primary target
  - head_link           - Balance perturbation
  - [left/right]_shoulder_pitch_link - Arm perturbation

INTEGRATION POINTS:
  1. Add config parameters to YAML files
  2. Parse in environment __init__()
  3. Map body names to IDs
  4. Apply forces in _pre_physics_step() or _step_sim()
  5. Use Isaac Gym force API: apply_rigid_body_force_at_pos_tensor()

================================================================================
4. TRAINING WORKFLOW
================================================================================

ENTRY POINT: python mimickit/run.py [arguments]

TRAINING COMMAND EXAMPLE:
  python mimickit/run.py \
    --mode train \
    --num_envs 4096 \
    --env_config data/envs/amp_g1_env.yaml \
    --agent_config data/agents/amp_g1_agent.yaml \
    --visualize false \
    --out_model_file output/g1_model.pt

STEP-BY-STEP:
  1. run.py loads arguments
  2. build_env() creates environment from YAML config
  3. build_agent() creates agent from YAML config
  4. agent.train_model() main training loop:
     - Collect experience: env.step(action)
     - Compute rewards
     - Train networks
     - Save checkpoints

PER-STEP PHYSICS:
  env.step(action)
    ├─ _pre_physics_step(action)       [Apply motor commands]
    ├─ _physics_step()                 [Run physics simulation]
    └─ _post_physics_step()            [Update obs/rewards/done]

================================================================================
5. G1 ROBOT CONFIGURATION
================================================================================

DEFINITION: /data/assets/g1/g1.xml (MuJoCo format, 305 lines)

UPPER BODY STRUCTURE:
  Pelvis (base)
  └─ Waist (3 DOF: yaw, roll, pitch)
     └─ Torso (mass 9.598 kg)
        ├─ Head (sphere collision, r=0.06m)
        ├─ Left Shoulder Chain (3 DOF)
        │  └─ Left Elbow (1 DOF) → Wrist (3 DOF) → Hand
        └─ Right Shoulder Chain (3 DOF)
           └─ Right Elbow (1 DOF) → Wrist (3 DOF) → Hand

AVAILABLE MOTIONS:
  - g1_walk.pkl
  - g1_run.pkl
  - g1_spinkick.pkl
  - g1_cartwheel.pkl
  - g1_speed_vault.pkl

CONFIG FILES:
  - /data/envs/amp_g1_env.yaml (environment settings)
  - /data/agents/amp_g1_agent.yaml (agent settings)

================================================================================
6. ADD TRAINING SPECIFICS
================================================================================

ADD = AMP + Differential Discriminator

KEY DIFFERENCE FROM AMP:
  AMP:  Discriminator learns real vs. agent motion
  ADD:  Discriminator learns zero vs. non-zero observation difference

OBSERVATION DIFFERENCE:
  obs_diff = disc_obs_demo - disc_obs
  (larger difference when agent doesn't match demo)

TRAINING:
  - Positive: Reward discriminator for obs_diff ≈ 0
  - Negative: Penalty for large obs_diff
  - Combined reward: task_weight * task_r + disc_weight * disc_r

IMPLEMENTATION FILES:
  - add_env.py       - Environment with demo observations
  - add_agent.py     - Agent training logic
  - add_model.py     - Network models with discriminator

================================================================================
7. FORCE APPLICATION IN ISAAC GYM
================================================================================

PHYSICS ENGINE: /mimickit/engines/isaac_gym_engine.py

FORCE MECHANISM:
  - Callback-based approach
  - set_apply_forces_callback(callback) registers Python callback
  - Called during physics step via _apply_cmd()

KEY API METHODS:
  engine.apply_rigid_body_force_at_pos_tensor()
    - Apply forces to specific bodies
    - Input: character ID, body ID, force vectors [num_envs, 3]

STATE QUERIES:
  engine.get_root_pos(char_id)       # Root position [num_envs, 3]
  engine.get_root_rot(char_id)       # Root rotation [num_envs, 4] (quat)
  engine.get_body_pos(char_id)       # All body positions [num_envs, num_bodies, 3]
  engine.get_contact_forces(char_id) # Contact forces

STATE SETTING:
  engine.set_root_pos()
  engine.set_root_vel()
  engine.set_root_ang_vel()
  engine.set_dof_state()

================================================================================
8. KEY FILES REFERENCE
================================================================================

PHYSICS & ENVIRONMENT:
  engines/isaac_gym_engine.py        - Physics wrapper, force callback location
  envs/sim_env.py                    - Core step loop
  envs/deepmimic_env.py              - Base motion imitation environment
  envs/amp_env.py                    - AMP-specific environment
  envs/add_env.py                    - ADD-specific environment

LEARNING:
  learning/ppo_agent.py              - PPO training agent
  learning/amp_agent.py              - AMP discriminator training
  learning/add_agent.py              - ADD differential training
  learning/ppo_model.py              - Network models
  learning/add_model.py              - ADD-specific models

CONFIGURATION & EXECUTION:
  run.py                             - Entry point
  envs/env_builder.py                - Environment factory
  learning/agent_builder.py          - Agent factory
  data/envs/*.yaml                   - Environment configs
  data/agents/*.yaml                 - Agent configs

UTILITIES:
  util/torch_util.py                 - Quaternion/rotation math
  util/arg_parser.py                 - Config file parsing
  anim/motion_lib.py                 - Motion data loading
  anim/kin_char_model.py             - Forward kinematics

================================================================================
9. CONFIGURATION EXAMPLES
================================================================================

ENVIRONMENT CONFIG (amp_g1_env.yaml):
```yaml
env:
  char_file: "data/assets/g1/g1.xml"
  episode_length: 10.0
  motion_file: "data/motions/g1/g1_walk.pkl"
  
  # For random forces:
  enable_random_force: true
  random_force_scale: [5.0, 5.0, 5.0]  # x, y, z in Newtons
  random_force_bodies: ["torso_link", "head_link"]
  random_force_probability: 0.5
```

AGENT CONFIG (amp_g1_agent.yaml):
```yaml
agent_name: "AMP"
model:
  actor_net: "fc_2layers_1024units"
  critic_net: "fc_2layers_1024units"
  disc_net: "fc_2layers_1024units"
disc_reward_weight: 1.0
task_reward_weight: 0.0
```

================================================================================
10. IMPLEMENTATION CHECKLIST FOR RANDOM FORCES
================================================================================

[ ] Read CODEBASE_OVERVIEW.md and KEY_INTEGRATION_POINTS.md
[ ] Modify deepmimic_env.py to parse random force config
[ ] Add body ID mapping for target bodies
[ ] Implement _apply_random_forces() method
[ ] Call force application in step() or _step_sim()
[ ] Test force tensor shapes match Isaac Gym API
[ ] Add force parameters to YAML configs
[ ] Test with visualization (visualize: true)
[ ] Run training with enable_random_force: true
[ ] Compare training curves with/without random forces
[ ] Document results and hyperparameters

================================================================================
11. USEFUL COMMANDS
================================================================================

# View a motion
python mimickit/run.py --mode test --arg_file args/view_motion_g1_args.txt \
  --visualize true

# Train AMP on G1 walk
python mimickit/run.py --arg_file args/amp_g1_args.txt --visualize false \
  --num_envs 4096

# Train ADD on G1 walk
python mimickit/run.py --arg_file args/add_g1_args.txt --visualize false \
  --num_envs 4096

# Test trained model
python mimickit/run.py --mode test --arg_file args/amp_g1_args.txt \
  --model_file output/g1_model.pt --visualize true

# View tensorboard logs
tensorboard --logdir=output/ --port=6006

================================================================================
12. CONTACT & DOCUMENTATION
================================================================================

CODEBASE REPOSITORY STRUCTURE:
  - 69 Python files total
  - 4 main packages: anim, engines, envs, learning
  - ~4000 lines of core code

ADDITIONAL RESOURCES:
  - CODEBASE_OVERVIEW.md (this directory)
  - KEY_INTEGRATION_POINTS.md (this directory)
  - Original README.md with paper citations

RESEARCH PAPERS:
  - DeepMimic: Example-guided Deep RL (Peng et al. 2018)
  - AMP: Adversarial Motion Priors (Peng et al. 2021)
  - ADD: Adversarial Differential Discriminators (Zhang et al. 2025)
  - ASE: Large-scale Reusable Skill Embeddings (Peng et al. 2022)

================================================================================
END OF QUICK REFERENCE
================================================================================
